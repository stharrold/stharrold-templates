# Self-optimizing 4-tier cascading AI agent workflow implementation v2

Production-ready implementation with standardized two-digit numbering for all stages and components.

## Stage numbering convention
```
00-09: Tier 0 (BMAD/Autogen)
10-19: Tier 1 (BAML transformation)
20-29: Tier 2 (Spec-kit to DSPy)
30-39: Tier 3 (DSPy execution)
40-49: Training/optimization
50-59: Archival/restoration
60-69: Monitoring/metrics
70-79: Resource management
80-89: Infrastructure
90-99: Utilities
```

## Core architecture

### 00_hierarchical_namespace.py
```python
from datetime import datetime
from pathlib import Path
import zlib
import pickle
import numpy as np
import redis
import asyncio
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import itertools
import dspy
from dspy.teleprompt import MIPROv2
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
from ollama import Embeddings as OllamaEmbeddings

@dataclass
class ArchitectureConfig:
    """01_architecture_config.pkl - Programmatically saved/restored"""
    version: str = "2.0.0"
    tiers: int = 4
    nodes_per_tier: List[int] = field(default_factory=lambda: [1, 1, 1, 1])
    embedding_model: str = "nomic-embed-text:v1.5"
    embedding_dim: int = 256
    redis_config: Dict = field(default_factory=dict)
    memory_threshold_percent: float = 80.0
    max_training_hours: int = 24
    convergence_threshold: float = 0.001
    created_at: str = field(default_factory=lambda: datetime.utcnow().strftime("%Y%m%dT%H%M%SZ"))
    
    def save(self, path: Path):
        """Save to 01_architecture_config.pkl"""
        temp_path = path.with_suffix('.tmp')
        with open(temp_path, 'wb') as f:
            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)
        temp_path.replace(path)
    
    @classmethod
    def load(cls, path: Path) -> 'ArchitectureConfig':
        with open(path, 'rb') as f:
            config = pickle.load(f)
        if not isinstance(config, cls):
            raise ValueError("Invalid configuration file")
        return config

class HierarchicalNamespace:
    """02_namespace_manager.py"""
    
    @staticmethod
    def generate_session_id() -> str:
        return datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    
    @staticmethod
    def create_path(base_dir: Path, session_id: str, node_group: Tuple[int, ...]) -> Path:
        group_size = len(node_group)
        group_id = "_".join([f"{id:02d}" for id in node_group])  # Two-digit formatting
        return base_dir / session_id / f"group_size_{group_size:02d}" / f"nodes_{group_id}"
```

## Training data architecture

### 10_training_data_manager.py
```python
@dataclass
class CascadeTrace:
    """11_cascade_trace.py - Human-validated cascade trace"""
    record_id: str
    agent_chain_inputs: Dict[str, np.ndarray]
    agent_chain_outputs: Dict[str, np.ndarray]
    agent_chain_metrics: Dict[str, Dict[str, float]]
    validated_json: Dict[str, Any]
    
class TrainingDataManager:
    """12_data_manager.py - UMAP dimensionality reduction"""
    
    def __init__(self, config: ArchitectureConfig):
        self.config = config
        self.embed_model = OllamaEmbeddings(
            model=config.embedding_model,
            model_kwargs={"dimensions": 256}
        )
        self.redis = redis.Redis(
            decode_responses=False,
            socket_keepalive=True,
            socket_connect_timeout=2,
            **config.redis_config
        )
        
        # Initialize UMAP for 256â†’64 reduction
        import umap
        self.umap_model = umap.UMAP(
            n_components=64,
            n_neighbors=15,
            min_dist=0.1,
            metric='cosine',
            random_state=42
        )
        self.umap_fitted = False
        self.embed_to_orthog_map = {}
        self.orthog_to_embed_map = {}
        
    def fit_umap(self, training_embeddings: np.ndarray):
        """13_umap_fitting.py"""
        self.umap_model.fit(training_embeddings)
        self.umap_fitted = True
        import joblib
        joblib.dump(self.umap_model, Path("/mnt/models/14_umap_256_to_64.pkl"))
        
    def store_trace_binary(self, trace: CascadeTrace) -> None:
        """15_trace_storage.py - Store with UMAP-reduced 64-dim embeddings"""
        if not self.umap_fitted:
            raise ValueError("UMAP model not fitted")
        
        packed_inputs = {}
        packed_outputs = {}
        
        for k, v in trace.agent_chain_inputs.items():
            embed_id = f"emb_{trace.record_id}_{k:02d}_in"  # Two-digit formatting
            self.redis.set(f"embed:orig:{embed_id}", v.astype(np.float32).tobytes(), ex=86400)
            
            v_64 = self.umap_model.transform(v.reshape(1, -1))[0]
            orthog_id = f"orth_{trace.record_id}_{k:02d}_in"
            
            self.embed_to_orthog_map[embed_id] = orthog_id
            self.orthog_to_embed_map[orthog_id] = (embed_id, v_64)
            
            packed_inputs[k] = v_64.astype(np.float16).tobytes()
        
        for k, v in trace.agent_chain_outputs.items():
            embed_id = f"emb_{trace.record_id}_{k:02d}_out"
            self.redis.set(f"embed:orig:{embed_id}", v.astype(np.float32).tobytes(), ex=86400)
            
            v_64 = self.umap_model.transform(v.reshape(1, -1))[0]
            orthog_id = f"orth_{trace.record_id}_{k:02d}_out"
            
            self.embed_to_orthog_map[embed_id] = orthog_id
            self.orthog_to_embed_map[orthog_id] = (embed_id, v_64)
            
            packed_outputs[k] = v_64.astype(np.float16).tobytes()
        
        self._store_mappings(trace.record_id)
        
        compressed_data = zlib.compress(pickle.dumps({
            'metrics': trace.agent_chain_metrics,
            'validated': trace.validated_json
        }), level=9)
        
        pipe = self.redis.pipeline()
        pipe.hset(f"trace:{trace.record_id}:inputs", mapping=packed_inputs)
        pipe.hset(f"trace:{trace.record_id}:outputs", mapping=packed_outputs)
        pipe.set(f"trace:{trace.record_id}:data", compressed_data)
        pipe.expire(f"trace:{trace.record_id}:inputs", 86400)
        pipe.expire(f"trace:{trace.record_id}:outputs", 86400)
        pipe.expire(f"trace:{trace.record_id}:data", 86400)
        pipe.execute()
    
    def _store_mappings(self, record_id: str):
        """16_mapping_storage.py"""
        self.redis.hset(f"map:embed_orthog:{record_id}", 
                       mapping={k: v for k, v in self.embed_to_orthog_map.items() 
                               if record_id in k})
        
        orthog_data = {}
        for orthog_id, (embed_id, coeffs) in self.orthog_to_embed_map.items():
            if record_id in orthog_id:
                orthog_data[orthog_id] = pickle.dumps((embed_id, coeffs))
        
        if orthog_data:
            self.redis.hset(f"map:orthog_embed:{record_id}", mapping=orthog_data)
        
        self.redis.expire(f"map:embed_orthog:{record_id}", 86400)
        self.redis.expire(f"map:orthog_embed:{record_id}", 86400)
    
    def build_io_matrix(self, node_group: Tuple[int, ...], 
                       traces: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """17_matrix_builder.py"""
        n_traces = len(traces)
        n_nodes = len(node_group)
        
        input_matrix = np.zeros((n_traces * n_nodes, 64), dtype=np.float32)
        output_matrix = np.zeros((n_traces * n_nodes, 64), dtype=np.float32)
        
        pipe = self.redis.pipeline()
        for trace_id in traces:
            for node_id in node_group:
                pipe.hget(f"trace:{trace_id}:inputs", str(node_id))
                pipe.hget(f"trace:{trace_id}:outputs", str(node_id))
        
        results = pipe.execute()
        
        idx = 0
        for i in range(0, len(results), 2 * n_nodes):
            for j in range(n_nodes):
                input_bytes = results[i + j*2]
                output_bytes = results[i + j*2 + 1]
                
                if input_bytes and output_bytes:
                    input_matrix[idx] = np.frombuffer(input_bytes, dtype=np.float16).astype(np.float32)
                    output_matrix[idx] = np.frombuffer(output_bytes, dtype=np.float16).astype(np.float32)
                idx += 1
        
        return input_matrix[:idx], output_matrix[:idx]
```

## Bayesian combinatorial training

### 40_bayesian_trainer.py
```python
class BayesianCombinatoricalTrainer:
    """41_combinatorial_trainer.py"""
    
    def __init__(self, config: ArchitectureConfig):
        self.config = config
        self.session_id = HierarchicalNamespace.generate_session_id()
        self.redis = redis.Redis(decode_responses=False, **config.redis_config)
        self.data_manager = TrainingDataManager(config)
        self.stop_event = asyncio.Event()
        
    async def train_all_groups_parallel(self, validated_traces: pd.DataFrame) -> Dict:
        """42_parallel_training.py"""
        start_time = asyncio.get_event_loop().time()
        results = {}
        
        all_groups = []
        for group_size in range(1, 5):
            all_groups.extend(itertools.combinations(range(4), group_size))
        
        tasks = []
        for node_group in all_groups:
            task = asyncio.create_task(
                self._train_node_group_with_timeout(node_group, validated_traces)
            )
            tasks.append((node_group, task))
        
        try:
            await asyncio.wait_for(
                asyncio.gather(*[t for _, t in tasks]),
                timeout=self.config.max_training_hours * 3600
            )
        except asyncio.TimeoutError:
            self.stop_event.set()
            for _, task in tasks:
                if not task.done():
                    task.cancel()
        
        for node_group, task in tasks:
            if task.done() and not task.cancelled():
                try:
                    results[node_group] = task.result()
                except Exception as e:
                    results[node_group] = {'error': str(e)}
        
        return {
            'session_id': self.session_id,
            'duration': asyncio.get_event_loop().time() - start_time,
            'results': results
        }
    
    async def _train_node_group_with_timeout(self, node_group: Tuple[int, ...], 
                                            traces: pd.DataFrame) -> Dict:
        """43_node_group_training.py"""
        group_id = "_".join([f"{id:02d}" for id in node_group])
        
        prior = await self._load_prior(group_id)
        optimizer = self._create_optimizer(node_group)
        
        iteration = 0
        
        while not self.stop_event.is_set():
            samples = self._weighted_sample(traces, prior)
            trace_ids = samples['record_id'].tolist()
            input_matrix, output_matrix = self.data_manager.build_io_matrix(
                node_group, trace_ids
            )
            
            trainset = self._create_trainset(input_matrix, output_matrix, samples)
            
            optimized = optimizer.compile(
                self._create_node_module(node_group),
                trainset=trainset,
                requires_permission_to_run=False
            )
            
            if hasattr(optimized, '_trace') and optimized._trace:
                if optimized._trace[-1].get('converged', False):
                    break
            
            score = optimized._trace[-1]['score'] if hasattr(optimized, '_trace') else 0.0
            
            posterior = self._update_posterior(prior, score)
            await self._save_posterior(group_id, posterior)
            prior = posterior
            
            iteration += 1
            
            if iteration % 10 == 0 and self.stop_event.is_set():
                break
        
        return {
            'node_group': node_group,
            'iterations': iteration,
            'final_score': score,
            'converged': optimized._trace[-1].get('converged', False) if hasattr(optimized, '_trace') else False
        }
    
    def _create_optimizer(self, node_group: Tuple[int, ...]) -> MIPROv2:
        """44_optimizer_creation.py"""
        return MIPROv2(
            metric=self._cosine_similarity_metric,
            auto="heavy",
            num_candidates=12 + len(node_group) * 2,
            init_temperature=0.7,
            minibatch=True,
            minibatch_size=min(50, len(node_group) * 25),
            minibatch_full_eval_steps=3,
            num_trials=40,
            program_aware_proposer=True,
            data_aware_proposer=True,
            fewshot_aware_proposer=True,
            track_stats=True
        )
    
    def _cosine_similarity_metric(self, example, prediction, trace=None):
        """45_similarity_metric.py"""
        pred_embedding = prediction.output_embedding
        true_embedding = example.validated_embedding
        
        similarity = np.dot(pred_embedding, true_embedding) / (
            np.linalg.norm(pred_embedding) * np.linalg.norm(true_embedding)
        )
        
        if trace is not None:
            return similarity > 0.95
        return similarity
    
    def _weighted_sample(self, traces: pd.DataFrame, 
                        prior: np.ndarray) -> pd.DataFrame:
        """46_weighted_sampling.py"""
        weights = np.exp(prior) / np.sum(np.exp(prior))
        n_samples = min(100, len(traces))
        indices = np.random.choice(len(traces), size=n_samples, 
                                 replace=True, p=weights)
        return traces.iloc[indices]
    
    async def _load_prior(self, group_id: str) -> np.ndarray:
        """47_prior_loading.py"""
        key = f"dist:{group_id}:latest"
        prior_bytes = self.redis.get(key)
        
        if prior_bytes:
            return np.frombuffer(prior_bytes, dtype=np.float16)
        else:
            return np.ones(100, dtype=np.float16) / 100
    
    async def _save_posterior(self, group_id: str, posterior: np.ndarray):
        """48_posterior_saving.py"""
        key = f"dist:{group_id}:latest"
        self.redis.set(key, posterior.astype(np.float16).tobytes())
        self.redis.expire(key, 86400 * 7)
```

## Container memory management

### 70_memory_manager.py
```python
class ContainerMemoryManager:
    """71_container_memory.py"""
    
    @staticmethod
    def get_container_memory_stats() -> Dict[str, int]:
        """72_memory_stats.py"""
        stats = {}
        
        try:
            with open('/sys/fs/cgroup/memory.current', 'r') as f:
                stats['current'] = int(f.read().strip())
            with open('/sys/fs/cgroup/memory.max', 'r') as f:
                limit = f.read().strip()
                stats['limit'] = int(limit) if limit != 'max' else psutil.virtual_memory().total
            with open('/sys/fs/cgroup/memory.stat', 'r') as f:
                for line in f:
                    if 'file' in line:
                        parts = line.split()
                        stats['cache'] = int(parts[1])
        except FileNotFoundError:
            try:
                with open('/sys/fs/cgroup/memory/memory.usage_in_bytes', 'r') as f:
                    stats['current'] = int(f.read().strip())
                with open('/sys/fs/cgroup/memory/memory.limit_in_bytes', 'r') as f:
                    stats['limit'] = int(f.read().strip())
                with open('/sys/fs/cgroup/memory/memory.stat', 'r') as f:
                    for line in f:
                        if line.startswith('cache'):
                            stats['cache'] = int(line.split()[1])
            except FileNotFoundError:
                mem = psutil.virtual_memory()
                stats = {
                    'current': mem.used,
                    'limit': mem.total,
                    'cache': mem.cached if hasattr(mem, 'cached') else 0
                }
        
        stats['percent'] = (stats['current'] / stats['limit']) * 100
        stats['available'] = stats['limit'] - stats['current'] + stats.get('cache', 0)
        
        return stats
    
    @classmethod
    def check_memory_gate(cls, threshold_percent: float = 80.0) -> bool:
        """73_memory_gate.py"""
        stats = cls.get_container_memory_stats()
        return stats['percent'] < threshold_percent
```

## Automated archival and restoration

### 50_archival_system.py
```python
class TrainingArchivalSystem:
    """51_archival_manager.py"""
    
    def __init__(self, config: ArchitectureConfig):
        self.config = config
        self.base_archive_dir = Path("/mnt/archives")
        self.base_archive_dir.mkdir(parents=True, exist_ok=True)
        
    async def archive_training_session(self, session_id: str, 
                                      results: Dict) -> List[Path]:
        """52_session_archival.py"""
        archive_paths = []
        
        results_by_size = {1: [], 2: [], 3: [], 4: []}
        for node_group, group_results in results['results'].items():
            size = len(node_group)
            results_by_size[size].append((node_group, group_results))
        
        for group_size, size_results in results_by_size.items():
            if not size_results:
                continue
                
            archive_path = self.base_archive_dir / f"{session_id}_size{group_size:02d}.tar.gz"
            
            temp_dir = Path(f"/tmp/{session_id}_size{group_size:02d}")
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            try:
                config_path = temp_dir / "01_architecture_config.pkl"
                self.config.save(config_path)
                
                for node_group, group_results in size_results:
                    group_dir = HierarchicalNamespace.create_path(
                        temp_dir, session_id, node_group
                    )
                    group_dir.mkdir(parents=True, exist_ok=True)
                    
                    if 'posterior' in group_results:
                        posterior_file = group_dir / "53_posterior.npy"
                        np.save(posterior_file, group_results['posterior'].astype(np.float32))
                    
                    self._save_reference_files(group_dir, node_group)
                    
                    if 'io_matrix' in group_results:
                        self._save_io_matrix_parquet(
                            group_dir / "54_io_matrix.parquet",
                            group_results['io_matrix']
                        )
                
                import tarfile
                with tarfile.open(archive_path, "w:gz", compresslevel=9) as tar:
                    tar.add(temp_dir, arcname=f"{session_id}_size{group_size:02d}")
                
                archive_paths.append(archive_path)
                
            finally:
                import shutil
                shutil.rmtree(temp_dir, ignore_errors=True)
        
        return archive_paths
    
    async def restore_from_archive(self, archive_path: Path) -> Dict:
        """55_archive_restoration.py"""
        if not archive_path.exists():
            raise FileNotFoundError(f"Archive not found: {archive_path}")
        
        filename = archive_path.stem
        if '_size' not in filename:
            raise ValueError("Invalid archive filename format")
        
        group_size = int(filename.split('_size')[1])
        session_id = filename.split('_size')[0]
        
        temp_dir = Path(f"/tmp/restore_{datetime.utcnow().timestamp()}")
        
        try:
            import tarfile
            with tarfile.open(archive_path, "r:gz") as tar:
                tar.extractall(temp_dir)
            
            session_dirs = list(temp_dir.glob("*"))
            if not session_dirs:
                raise ValueError("Invalid archive structure")
            
            session_dir = session_dirs[0]
            
            config_path = session_dir / "01_architecture_config.pkl"
            restored_config = ArchitectureConfig.load(config_path)
            
            if restored_config.version != self.config.version:
                restored_config = self._impute_config_differences(
                    restored_config, self.config
                )
            
            results = {
                'session_id': session_id, 
                'group_size': group_size,
                'results': {}
            }
            
            size_dir = session_dir / f"group_size_{group_size:02d}"
            if size_dir.exists():
                for node_dir in size_dir.glob("nodes_*"):
                    node_ids_str = node_dir.name.split("_")[1:]
                    node_ids = tuple(map(int, [s for s in node_ids_str if s]))
                    
                    posterior_file = node_dir / "53_posterior.npy"
                    if posterior_file.exists():
                        posterior = np.load(posterior_file)
                    else:
                        posterior = self._impute_posterior(node_ids, results)
                    
                    io_matrix_file = node_dir / "54_io_matrix.parquet"
                    if io_matrix_file.exists():
                        io_matrix = pq.read_table(io_matrix_file).to_pandas()
                    else:
                        io_matrix = None
                    
                    results['results'][node_ids] = {
                        'posterior': posterior,
                        'io_matrix': io_matrix
                    }
            
            return {
                'config': restored_config,
                'results': results
            }
            
        finally:
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    def _impute_config_differences(self, old_config: ArchitectureConfig, 
                                   new_config: ArchitectureConfig) -> ArchitectureConfig:
        """56_config_imputation.py"""
        imputed = ArchitectureConfig()
        
        for field in ['embedding_model', 'embedding_dim', 'convergence_threshold']:
            if hasattr(old_config, field):
                setattr(imputed, field, getattr(old_config, field))
        
        if old_config.tiers != new_config.tiers:
            imputed.tiers = min(old_config.tiers, new_config.tiers)
        
        if old_config.nodes_per_tier != new_config.nodes_per_tier:
            imputed.nodes_per_tier = [
                min(old, new) for old, new in 
                zip(old_config.nodes_per_tier, new_config.nodes_per_tier)
            ]
        
        return imputed
    
    def _impute_posterior(self, node_ids: Tuple[int, ...], 
                         existing_results: Dict) -> np.ndarray:
        """57_posterior_imputation.py"""
        min_distance = float('inf')
        nearest_posterior = None
        
        for existing_ids, data in existing_results['results'].items():
            if 'posterior' in data:
                distance = len(set(node_ids) ^ set(existing_ids))
                if distance < min_distance:
                    min_distance = distance
                    nearest_posterior = data['posterior']
        
        if nearest_posterior is not None:
            return nearest_posterior
        else:
            return np.ones(100, dtype=np.float16) / 100
    
    def _save_reference_files(self, group_dir: Path, node_group: Tuple[int, ...]):
        """58_reference_files.py"""
        chain_names = {
            0: "agent-00-autogen-bmad",
            1: "agent-10-baml-transform", 
            2: "agent-20-speckit-dspy",
            3: "agent-30-dspy-execution"
        }
        
        mapping = {f"{node_id:02d}": chain_names[node_id] for node_id in node_group}
        
        ref_file = group_dir / "59_node_chain_mapping.json"
        with open(ref_file, 'w') as f:
            json.dump(mapping, f, indent=2)
    
    def _save_io_matrix_parquet(self, path: Path, matrix_data: Dict):
        """54_io_matrix_storage.py"""
        df = pd.DataFrame(matrix_data)
        table = pa.Table.from_pandas(df)
        pq.write_table(
            table, path,
            compression='zstd',
            compression_level=22,
            use_dictionary=True,
            data_page_size=1024*1024
        )
```

## Production orchestration

### 80_orchestrator.py
```python
class ProductionTrainingOrchestrator:
    """81_production_orchestrator.py"""
    
    def __init__(self):
        self.config = ArchitectureConfig()
        self.trainer = BayesianCombinatoricalTrainer(self.config)
        self.archiver = TrainingArchivalSystem(self.config)
        self.memory_manager = ContainerMemoryManager()
        
    async def run_training_session(self, mode: str = "mode-training") -> Dict:
        """82_training_session.py"""
        
        if not self.memory_manager.check_memory_gate(self.config.memory_threshold_percent):
            raise MemoryError("Insufficient memory for training")
        
        traces = await self._load_validated_traces()
        results = await self.trainer.train_all_groups_parallel(traces)
        archive_paths = await self.archiver.archive_training_session(
            results['session_id'], results
        )
        
        await self._cleanup_redis_data(results['session_id'])
        
        return {
            'session_id': results['session_id'],
            'archive_paths': [str(p) for p in archive_paths],
            'duration': results['duration'],
            'converged_groups': sum(
                1 for r in results['results'].values() 
                if r.get('converged', False)
            )
        }
    
    async def warm_start_from_archive(self, archive_path: Path) -> None:
        """83_warm_start.py"""
        restored = await self.archiver.restore_from_archive(archive_path)
        
        for node_group, data in restored['results']['results'].items():
            if 'posterior' in data:
                group_id = "_".join([f"{id:02d}" for id in node_group])
                key = f"dist:{group_id}:latest"
                self.trainer.redis.set(
                    key, 
                    data['posterior'].astype(np.float16).tobytes()
                )
    
    async def _load_validated_traces(self) -> pd.DataFrame:
        """84_trace_loading.py"""
        traces_path = Path("/mnt/validated_traces/85_latest.parquet")
        if not traces_path.exists():
            raise FileNotFoundError("No validated traces found")
        return pd.read_parquet(traces_path)
    
    async def _cleanup_redis_data(self, session_id: str):
        """86_redis_cleanup.py"""
        pattern = f"*{session_id}*"
        cursor = 0
        
        while True:
            cursor, keys = self.trainer.redis.scan(
                cursor, match=pattern, count=1000
            )
            
            if keys:
                self.trainer.redis.delete(*keys)
            
            if cursor == 0:
                break

# Entry point
async def main():
    orchestrator = ProductionTrainingOrchestrator()
    
    latest_archive = Path("/mnt/archives").glob("*.tar.gz")
    latest_archive = max(latest_archive, key=lambda p: p.stat().st_mtime, default=None)
    
    if latest_archive:
        print(f"Warm starting from {latest_archive}")
        await orchestrator.warm_start_from_archive(latest_archive)
    
    result = await orchestrator.run_training_session(mode="mode-training")
    print(f"Training completed: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Docker deployment

```yaml
# 90_docker-compose.yml
version: '3.8'

services:
  redis-primary:
    image: redis:7.2-alpine
    command: >
      redis-server
      --maxmemory 8gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    deploy:
      resources:
        limits:
          memory: 10G
        reservations:
          memory: 8G
  
  training-orchestrator:
    build: .
    environment:
      - MODE=mode-training
      - REDIS_URL=redis://redis-primary:6379
      - OLLAMA_HOST=http://ollama:11434
      - MEMORY_THRESHOLD=80
      - MAX_TRAINING_HOURS=24
      - CONVERGENCE_THRESHOLD=0.001
    volumes:
      - ./85_validated_traces:/mnt/validated_traces:ro
      - ./91_archives:/mnt/archives
      - ./92_training-data:/mnt/training
      - ./14_models:/mnt/models
    depends_on:
      - redis-primary
      - ollama
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 12G
  
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama-models:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 6G
    command: serve

  archival-cron:
    build: .
    command: python -m 93_archival_service
    environment:
      - ARCHIVE_SCHEDULE="0 */6 * * *"
      - REDIS_URL=redis://redis-primary:6379
    volumes:
      - ./91_archives:/mnt/archives
      - ./92_training-data:/mnt/training
    depends_on:
      - redis-primary

volumes:
  redis-data:
  92_training-data:
  ollama-models:
```

## Summary

All files and stages now follow consistent two-digit numbering:
- **00-09**: Core architecture and namespaces
- **10-19**: Training data management and UMAP
- **40-49**: Bayesian training components
- **50-59**: Archival and restoration
- **70-79**: Resource management
- **80-89**: Production orchestration
- **90-99**: Deployment and utilities

This ensures clear correlation between related components and maintains hierarchical organization.

### Hierarchical namespace structure
```python
from datetime import datetime
from pathlib import Path
import zlib
import pickle
import numpy as np
import redis
import asyncio
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
import itertools
import dspy
from dspy.teleprompt import MIPROv2
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
from ollama import Embeddings as OllamaEmbeddings

@dataclass
class ArchitectureConfig:
    """Programmatically saved/restored architecture configuration"""
    version: str = "2.0.0"
    tiers: int = 4
    nodes_per_tier: List[int] = field(default_factory=lambda: [1, 1, 1, 1])
    embedding_model: str = "nomic-embed-text:v1.5"
    embedding_dim: int = 256  # Direct 256-dim output, no quantization needed
    redis_config: Dict = field(default_factory=dict)
    memory_threshold_percent: float = 80.0
    max_training_hours: int = 24
    convergence_threshold: float = 0.001
    created_at: str = field(default_factory=lambda: datetime.utcnow().strftime("%Y%m%dT%H%M%SZ"))
    
    def save(self, path: Path):
        """Save configuration atomically"""
        temp_path = path.with_suffix('.tmp')
        with open(temp_path, 'wb') as f:
            pickle.dump(self, f, protocol=pickle.HIGHEST_PROTOCOL)
        temp_path.replace(path)
    
    @classmethod
    def load(cls, path: Path) -> 'ArchitectureConfig':
        """Load configuration with validation"""
        with open(path, 'rb') as f:
            config = pickle.load(f)
        if not isinstance(config, cls):
            raise ValueError("Invalid configuration file")
        return config

class HierarchicalNamespace:
    """ISO 8601 hierarchical namespace management"""
    
    @staticmethod
    def generate_session_id() -> str:
        """Generate ISO 8601 session identifier"""
        return datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    
    @staticmethod
    def create_path(base_dir: Path, session_id: str, node_group: Tuple[int, ...]) -> Path:
        """Create hierarchical path for node group"""
        group_size = len(node_group)
        group_id = "_".join(map(str, node_group))
        return base_dir / session_id / f"group_size_{group_size}" / f"nodes_{group_id}"
```

## Training data architecture

### Cascade trace storage with embeddings
```python
@dataclass
class CascadeTrace:
    """Human-validated cascade trace with embeddings"""
    record_id: str
    agent_chain_inputs: Dict[str, np.ndarray]  # Agent ID -> input embedding
    agent_chain_outputs: Dict[str, np.ndarray]  # Agent ID -> output embedding
    agent_chain_metrics: Dict[str, Dict[str, float]]  # Agent ID -> metrics
    validated_json: Dict[str, Any]  # Human-annotated ground truth
    
class TrainingDataManager:
    """Manages training data with UMAP dimensionality reduction"""
    
    def __init__(self, config: ArchitectureConfig):
        self.config = config
        # Configure Ollama for 256-dim output
        self.embed_model = OllamaEmbeddings(
            model=config.embedding_model,
            model_kwargs={"dimensions": 256}
        )
        self.redis = redis.Redis(
            decode_responses=False,
            socket_keepalive=True,
            socket_connect_timeout=2,
            **config.redis_config
        )
        
        # Initialize UMAP for 256â†’64 reduction
        import umap
        self.umap_model = umap.UMAP(
            n_components=64,
            n_neighbors=15,
            min_dist=0.1,
            metric='cosine',
            random_state=42
        )
        self.umap_fitted = False
        self.embed_to_orthog_map = {}  # Maps embed_id to orthog_id
        self.orthog_to_embed_map = {}  # Maps orthog_id to (embed_id, coefficients)
        
    def fit_umap(self, training_embeddings: np.ndarray):
        """Fit UMAP on representative embeddings"""
        self.umap_model.fit(training_embeddings)
        self.umap_fitted = True
        # Save UMAP model
        import joblib
        joblib.dump(self.umap_model, Path("/mnt/models/umap_256_to_64.pkl"))
        
    def store_trace_binary(self, trace: CascadeTrace) -> None:
        """Store trace with UMAP-reduced 64-dim embeddings"""
        if not self.umap_fitted:
            raise ValueError("UMAP model not fitted. Call fit_umap first.")
        
        # Process embeddings through UMAP
        packed_inputs = {}
        packed_outputs = {}
        
        for k, v in trace.agent_chain_inputs.items():
            # Original 256-dim embedding
            embed_id = f"emb_{trace.record_id}_{k}_in"
            # Store original for reconstruction mapping
            self.redis.set(f"embed:orig:{embed_id}", v.astype(np.float32).tobytes(), ex=86400)
            
            # UMAP reduction to 64-dim
            v_64 = self.umap_model.transform(v.reshape(1, -1))[0]
            orthog_id = f"orth_{trace.record_id}_{k}_in"
            
            # Store mappings
            self.embed_to_orthog_map[embed_id] = orthog_id
            self.orthog_to_embed_map[orthog_id] = (embed_id, v_64)
            
            # Store 64-dim as float16 (128 bytes)
            packed_inputs[k] = v_64.astype(np.float16).tobytes()
        
        for k, v in trace.agent_chain_outputs.items():
            embed_id = f"emb_{trace.record_id}_{k}_out"
            self.redis.set(f"embed:orig:{embed_id}", v.astype(np.float32).tobytes(), ex=86400)
            
            v_64 = self.umap_model.transform(v.reshape(1, -1))[0]
            orthog_id = f"orth_{trace.record_id}_{k}_out"
            
            self.embed_to_orthog_map[embed_id] = orthog_id
            self.orthog_to_embed_map[orthog_id] = (embed_id, v_64)
            
            packed_outputs[k] = v_64.astype(np.float16).tobytes()
        
        # Store mapping files in Redis
        self._store_mappings(trace.record_id)
        
        # Compress JSON data
        compressed_data = zlib.compress(pickle.dumps({
            'metrics': trace.agent_chain_metrics,
            'validated': trace.validated_json
        }), level=9)
        
        # Store in Redis
        pipe = self.redis.pipeline()
        pipe.hset(f"trace:{trace.record_id}:inputs", mapping=packed_inputs)
        pipe.hset(f"trace:{trace.record_id}:outputs", mapping=packed_outputs)
        pipe.set(f"trace:{trace.record_id}:data", compressed_data)
        pipe.expire(f"trace:{trace.record_id}:inputs", 86400)
        pipe.expire(f"trace:{trace.record_id}:outputs", 86400)
        pipe.expire(f"trace:{trace.record_id}:data", 86400)
        pipe.execute()
    
    def _store_mappings(self, record_id: str):
        """Store UMAP mapping relationships"""
        # Store embed->orthog mapping
        self.redis.hset(f"map:embed_orthog:{record_id}", 
                       mapping={k: v for k, v in self.embed_to_orthog_map.items() 
                               if record_id in k})
        
        # Store orthog->embed mapping with coefficients
        orthog_data = {}
        for orthog_id, (embed_id, coeffs) in self.orthog_to_embed_map.items():
            if record_id in orthog_id:
                orthog_data[orthog_id] = pickle.dumps((embed_id, coeffs))
        
        if orthog_data:
            self.redis.hset(f"map:orthog_embed:{record_id}", mapping=orthog_data)
        
        # Set expiration
        self.redis.expire(f"map:embed_orthog:{record_id}", 86400)
        self.redis.expire(f"map:orthog_embed:{record_id}", 86400)
    
    def build_io_matrix(self, node_group: Tuple[int, ...], 
                       traces: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """Build matrices from 64-dim UMAP embeddings"""
        n_traces = len(traces)
        n_nodes = len(node_group)
        
        # Pre-allocate 64-dim matrices
        input_matrix = np.zeros((n_traces * n_nodes, 64), dtype=np.float32)
        output_matrix = np.zeros((n_traces * n_nodes, 64), dtype=np.float32)
        
        # Batch fetch from Redis
        pipe = self.redis.pipeline()
        for trace_id in traces:
            for node_id in node_group:
                pipe.hget(f"trace:{trace_id}:inputs", str(node_id))
                pipe.hget(f"trace:{trace_id}:outputs", str(node_id))
        
        results = pipe.execute()
        
        # Unpack 64-dim embeddings
        idx = 0
        for i in range(0, len(results), 2 * n_nodes):
            for j in range(n_nodes):
                input_bytes = results[i + j*2]
                output_bytes = results[i + j*2 + 1]
                
                if input_bytes and output_bytes:
                    # Convert from float16 back to float32
                    input_matrix[idx] = np.frombuffer(input_bytes, dtype=np.float16).astype(np.float32)
                    output_matrix[idx] = np.frombuffer(output_bytes, dtype=np.float16).astype(np.float32)
                idx += 1
        
        return input_matrix[:idx], output_matrix[:idx]
```

## Bayesian combinatorial training system

### Parallel training with Redis-based distributions
```python
class BayesianCombinatoricalTrainer:
    """Trains all node group combinations with Bayesian updating"""
    
    def __init__(self, config: ArchitectureConfig):
        self.config = config
        self.session_id = HierarchicalNamespace.generate_session_id()
        self.redis = redis.Redis(decode_responses=False, **config.redis_config)
        self.data_manager = TrainingDataManager(config)
        self.stop_event = asyncio.Event()
        
    async def train_all_groups_parallel(self, validated_traces: pd.DataFrame) -> Dict:
        """Train all C(4,k) combinations simultaneously"""
        start_time = asyncio.get_event_loop().time()
        results = {}
        
        # Generate all 15 node group combinations
        all_groups = []
        for group_size in range(1, 5):
            all_groups.extend(itertools.combinations(range(4), group_size))
        
        # Launch parallel training tasks
        tasks = []
        for node_group in all_groups:
            task = asyncio.create_task(
                self._train_node_group_with_timeout(node_group, validated_traces)
            )
            tasks.append((node_group, task))
        
        # Wait for completion or timeout (24 hours)
        try:
            await asyncio.wait_for(
                asyncio.gather(*[t for _, t in tasks]),
                timeout=self.config.max_training_hours * 3600
            )
        except asyncio.TimeoutError:
            self.stop_event.set()
            # Gracefully stop all tasks
            for _, task in tasks:
                if not task.done():
                    task.cancel()
        
        # Collect results
        for node_group, task in tasks:
            if task.done() and not task.cancelled():
                try:
                    results[node_group] = task.result()
                except Exception as e:
                    results[node_group] = {'error': str(e)}
        
        return {
            'session_id': self.session_id,
            'duration': asyncio.get_event_loop().time() - start_time,
            'results': results
        }
    
    async def _train_node_group_with_timeout(self, node_group: Tuple[int, ...], 
                                            traces: pd.DataFrame) -> Dict:
        """Train single node group using MIPROv2 internal convergence"""
        group_id = "_".join(map(str, node_group))
        
        # Load prior from Redis (previous session's posterior)
        prior = await self._load_prior(group_id)
        
        # Setup MIPROv2 optimizer for this node group
        optimizer = self._create_optimizer(node_group)
        
        # Continuous training with MIPROv2's internal convergence detection
        iteration = 0
        
        while not self.stop_event.is_set():
            # Weighted sampling from traces
            samples = self._weighted_sample(traces, prior)
            
            # Build IO matrices
            trace_ids = samples['record_id'].tolist()
            input_matrix, output_matrix = self.data_manager.build_io_matrix(
                node_group, trace_ids
            )
            
            # Create training batch
            trainset = self._create_trainset(input_matrix, output_matrix, samples)
            
            # Optimize with MIPROv2 - it handles its own convergence
            optimized = optimizer.compile(
                self._create_node_module(node_group),
                trainset=trainset,
                requires_permission_to_run=False
            )
            
            # MIPROv2 internal convergence check
            if hasattr(optimized, '_trace') and optimized._trace:
                # Check if MIPROv2 has converged based on its internal metrics
                if optimized._trace[-1].get('converged', False):
                    break
            
            # Extract score from MIPROv2 trace
            score = optimized._trace[-1]['score'] if hasattr(optimized, '_trace') else 0.0
            
            # Update posterior (becomes next iteration's prior)
            posterior = self._update_posterior(prior, score)
            await self._save_posterior(group_id, posterior)
            prior = posterior
            
            iteration += 1
            
            # Check time limit every 10 iterations
            if iteration % 10 == 0 and self.stop_event.is_set():
                break
        
        return {
            'node_group': node_group,
            'iterations': iteration,
            'final_score': score,
            'converged': optimized._trace[-1].get('converged', False) if hasattr(optimized, '_trace') else False
        }
    
    def _create_optimizer(self, node_group: Tuple[int, ...]) -> MIPROv2:
        """Create MIPROv2 optimizer configured for accuracy"""
        return MIPROv2(
            metric=self._cosine_similarity_metric,
            auto="heavy",  # Maximum accuracy
            num_candidates=12 + len(node_group) * 2,  # Scale with complexity
            init_temperature=0.7,
            minibatch=True,
            minibatch_size=min(50, len(node_group) * 25),
            minibatch_full_eval_steps=3,
            num_trials=40,
            program_aware_proposer=True,
            data_aware_proposer=True,
            fewshot_aware_proposer=True,
            track_stats=True
        )
    
    def _cosine_similarity_metric(self, example, prediction, trace=None):
        """Compute cosine similarity between embeddings"""
        # Get embeddings (already computed and stored)
        pred_embedding = prediction.output_embedding
        true_embedding = example.validated_embedding
        
        # Compute cosine similarity
        similarity = np.dot(pred_embedding, true_embedding) / (
            np.linalg.norm(pred_embedding) * np.linalg.norm(true_embedding)
        )
        
        if trace is not None:
            return similarity > 0.95  # High threshold for accuracy
        return similarity
    
    def _weighted_sample(self, traces: pd.DataFrame, 
                        prior: np.ndarray) -> pd.DataFrame:
        """Continuous sampling weighted by probability distribution"""
        # Convert prior to sampling weights
        weights = np.exp(prior) / np.sum(np.exp(prior))
        
        # Sample with replacement
        n_samples = min(100, len(traces))
        indices = np.random.choice(len(traces), size=n_samples, 
                                 replace=True, p=weights)
        
        return traces.iloc[indices]
    
    async def _load_prior(self, group_id: str) -> np.ndarray:
        """Load prior from Redis (previous posterior)"""
        key = f"dist:{group_id}:latest"
        prior_bytes = self.redis.get(key)
        
        if prior_bytes:
            return np.frombuffer(prior_bytes, dtype=np.float16)
        else:
            # Uniform prior for first session
            return np.ones(100, dtype=np.float16) / 100
    
    async def _save_posterior(self, group_id: str, posterior: np.ndarray):
        """Save posterior to Redis for next session"""
        key = f"dist:{group_id}:latest"
        self.redis.set(key, posterior.astype(np.float16).tobytes())
        self.redis.expire(key, 86400 * 7)  # Keep for 7 days
```

## Container memory management

### Container-specific memory monitoring
```python
class ContainerMemoryManager:
    """Monitors container-specific memory with cgroup detection"""
    
    @staticmethod
    def get_container_memory_stats() -> Dict[str, int]:
        """Get container memory statistics"""
        stats = {}
        
        # Try cgroups v2 first
        try:
            with open('/sys/fs/cgroup/memory.current', 'r') as f:
                stats['current'] = int(f.read().strip())
            with open('/sys/fs/cgroup/memory.max', 'r') as f:
                limit = f.read().strip()
                stats['limit'] = int(limit) if limit != 'max' else psutil.virtual_memory().total
            with open('/sys/fs/cgroup/memory.stat', 'r') as f:
                for line in f:
                    if 'file' in line:
                        parts = line.split()
                        stats['cache'] = int(parts[1])
        except FileNotFoundError:
            # Fall back to cgroups v1
            try:
                with open('/sys/fs/cgroup/memory/memory.usage_in_bytes', 'r') as f:
                    stats['current'] = int(f.read().strip())
                with open('/sys/fs/cgroup/memory/memory.limit_in_bytes', 'r') as f:
                    stats['limit'] = int(f.read().strip())
                with open('/sys/fs/cgroup/memory/memory.stat', 'r') as f:
                    for line in f:
                        if line.startswith('cache'):
                            stats['cache'] = int(line.split()[1])
            except FileNotFoundError:
                # Not in container - use host stats
                mem = psutil.virtual_memory()
                stats = {
                    'current': mem.used,
                    'limit': mem.total,
                    'cache': mem.cached if hasattr(mem, 'cached') else 0
                }
        
        stats['percent'] = (stats['current'] / stats['limit']) * 100
        stats['available'] = stats['limit'] - stats['current'] + stats.get('cache', 0)
        
        return stats
    
    @classmethod
    def check_memory_gate(cls, threshold_percent: float = 80.0) -> bool:
        """Check if memory usage is below threshold"""
        stats = cls.get_container_memory_stats()
        return stats['percent'] < threshold_percent
```

## Automated archival and restoration

### Progressive archival with compression
```python
class TrainingArchivalSystem:
    """Automated archival with hierarchical namespaces"""
    
    def __init__(self, config: ArchitectureConfig):
        self.config = config
        self.base_archive_dir = Path("/mnt/archives")
        self.base_archive_dir.mkdir(parents=True, exist_ok=True)
        
    async def archive_training_session(self, session_id: str, 
                                      results: Dict) -> List[Path]:
        """Archive completed training session with separate archives per group-size"""
        archive_paths = []
        
        # Group results by size for separate archives
        results_by_size = {1: [], 2: [], 3: [], 4: []}
        for node_group, group_results in results['results'].items():
            size = len(node_group)
            results_by_size[size].append((node_group, group_results))
        
        # Create separate archive for each group size
        for group_size, size_results in results_by_size.items():
            if not size_results:
                continue
                
            archive_path = self.base_archive_dir / f"{session_id}_size{group_size}.tar.gz"
            
            # Create temporary directory for this group size
            temp_dir = Path(f"/tmp/{session_id}_size{group_size}")
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            try:
                # Save architecture config
                config_path = temp_dir / "architecture_config.pkl"
                self.config.save(config_path)
                
                # Save results for each node group of this size
                for node_group, group_results in size_results:
                    group_dir = HierarchicalNamespace.create_path(
                        temp_dir, session_id, node_group
                    )
                    group_dir.mkdir(parents=True, exist_ok=True)
                    
                    # Save posteriors (direct float32, no quantization)
                    if 'posterior' in group_results:
                        posterior_file = group_dir / "posterior.npy"
                        np.save(posterior_file, group_results['posterior'].astype(np.float32))
                    
                    # Save reference files
                    self._save_reference_files(group_dir, node_group)
                    
                    # Save IO matrices in Parquet format
                    if 'io_matrix' in group_results:
                        self._save_io_matrix_parquet(
                            group_dir / "io_matrix.parquet",
                            group_results['io_matrix']
                        )
                
                # Create compressed archive for this group size
                import tarfile
                with tarfile.open(archive_path, "w:gz", compresslevel=9) as tar:
                    tar.add(temp_dir, arcname=f"{session_id}_size{group_size}")
                
                archive_paths.append(archive_path)
                
            finally:
                # Clean up temporary directory
                import shutil
                shutil.rmtree(temp_dir, ignore_errors=True)
        
        return archive_paths
    
    async def restore_from_archive(self, archive_path: Path) -> Dict:
        """Restore training session from group-size specific archive"""
        if not archive_path.exists():
            raise FileNotFoundError(f"Archive not found: {archive_path}")
        
        # Extract group size from filename
        filename = archive_path.stem  # Remove .tar.gz
        if '_size' not in filename:
            raise ValueError("Invalid archive filename format")
        
        group_size = int(filename.split('_size')[1])
        session_id = filename.split('_size')[0]
        
        # Extract to temporary directory
        temp_dir = Path(f"/tmp/restore_{datetime.utcnow().timestamp()}")
        
        try:
            import tarfile
            with tarfile.open(archive_path, "r:gz") as tar:
                tar.extractall(temp_dir)
            
            # Find session directory
            session_dirs = list(temp_dir.glob("*"))
            if not session_dirs:
                raise ValueError("Invalid archive structure")
            
            session_dir = session_dirs[0]
            
            # Load architecture config
            config_path = session_dir / "architecture_config.pkl"
            restored_config = ArchitectureConfig.load(config_path)
            
            # Handle architecture mismatches with imputation
            if restored_config.version != self.config.version:
                restored_config = self._impute_config_differences(
                    restored_config, self.config
                )
            
            # Restore node group data for this specific size
            results = {
                'session_id': session_id, 
                'group_size': group_size,
                'results': {}
            }
            
            size_dir = session_dir / f"group_size_{group_size}"
            if size_dir.exists():
                for node_dir in size_dir.glob("nodes_*"):
                    node_ids = tuple(map(int, node_dir.name.split("_")[1:]))
                    
                    # Load posterior (direct float32)
                    posterior_file = node_dir / "posterior.npy"
                    if posterior_file.exists():
                        posterior = np.load(posterior_file)
                    else:
                        # Impute with nearest neighbor
                        posterior = self._impute_posterior(node_ids, results)
                    
                    # Load IO matrix
                    io_matrix_file = node_dir / "io_matrix.parquet"
                    if io_matrix_file.exists():
                        io_matrix = pq.read_table(io_matrix_file).to_pandas()
                    else:
                        io_matrix = None
                    
                    results['results'][node_ids] = {
                        'posterior': posterior,
                        'io_matrix': io_matrix
                    }
            
            return {
                'config': restored_config,
                'results': results
            }
            
        finally:
            # Clean up
            import shutil
            shutil.rmtree(temp_dir, ignore_errors=True)
    
    def _impute_config_differences(self, old_config: ArchitectureConfig, 
                                   new_config: ArchitectureConfig) -> ArchitectureConfig:
        """Impute differences when architectures don't match"""
        imputed = ArchitectureConfig()
        
        # Copy matching fields
        for field in ['embedding_model', 'embedding_dim', 'convergence_threshold']:
            if hasattr(old_config, field):
                setattr(imputed, field, getattr(old_config, field))
        
        # Handle version differences
        if old_config.tiers != new_config.tiers:
            # Use minimum common tiers
            imputed.tiers = min(old_config.tiers, new_config.tiers)
        
        # Handle node differences with nearest neighbor
        if old_config.nodes_per_tier != new_config.nodes_per_tier:
            imputed.nodes_per_tier = [
                min(old, new) for old, new in 
                zip(old_config.nodes_per_tier, new_config.nodes_per_tier)
            ]
        
        return imputed
    
    def _impute_posterior(self, node_ids: Tuple[int, ...], 
                         existing_results: Dict) -> np.ndarray:
        """Impute missing posterior using nearest neighbor"""
        # Find most similar node group
        min_distance = float('inf')
        nearest_posterior = None
        
        for existing_ids, data in existing_results['results'].items():
            if 'posterior' in data:
                # Compute Hamming distance
                distance = len(set(node_ids) ^ set(existing_ids))
                if distance < min_distance:
                    min_distance = distance
                    nearest_posterior = data['posterior']
        
        if nearest_posterior is not None:
            return nearest_posterior
        else:
            # Fall back to uniform
            return np.ones(100, dtype=np.float16) / 100
    
    def _save_reference_files(self, group_dir: Path, node_group: Tuple[int, ...]):
        """Save reference files for node group"""
        # Node chain names mapping
        chain_names = {
            0: "agent-00-autogen-bmad",
            1: "agent-10-baml-transform", 
            2: "agent-20-speckit-dspy",
            3: "agent-30-dspy-execution"
        }
        
        mapping = {node_id: chain_names[node_id] for node_id in node_group}
        
        ref_file = group_dir / "node_chain_mapping.json"
        with open(ref_file, 'w') as f:
            json.dump(mapping, f, indent=2)
    
    def _save_io_matrix_parquet(self, path: Path, matrix_data: Dict):
        """Save IO matrix in Parquet format for smallest size"""
        df = pd.DataFrame(matrix_data)
        
        # Use compression and column encoding
        table = pa.Table.from_pandas(df)
        pq.write_table(
            table, path,
            compression='zstd',  # Best compression ratio
            compression_level=22,  # Maximum compression
            use_dictionary=True,
            data_page_size=1024*1024  # 1MB pages
        )
```

## Production orchestration

### Main training coordinator
```python
class ProductionTrainingOrchestrator:
    """Production orchestrator with all optimizations"""
    
    def __init__(self):
        self.config = ArchitectureConfig()
        self.trainer = BayesianCombinatoricalTrainer(self.config)
        self.archiver = TrainingArchivalSystem(self.config)
        self.memory_manager = ContainerMemoryManager()
        
    async def run_training_session(self, mode: str = "mode-training") -> Dict:
        """Run complete training session with all features"""
        
        # Check memory gate
        if not self.memory_manager.check_memory_gate(self.config.memory_threshold_percent):
            raise MemoryError("Insufficient memory for training")
        
        # Load validated traces
        traces = await self._load_validated_traces()
        
        # Run parallel training
        results = await self.trainer.train_all_groups_parallel(traces)
        
        # Archive results
        archive_path = await self.archiver.archive_training_session(
            results['session_id'], results
        )
        
        # Clean up Redis
        await self._cleanup_redis_data(results['session_id'])
        
        return {
            'session_id': results['session_id'],
            'archive_path': str(archive_path),
            'duration': results['duration'],
            'converged_groups': sum(
                1 for r in results['results'].values() 
                if r.get('converged', False)
            )
        }
    
    async def warm_start_from_archive(self, archive_path: Path) -> None:
        """Warm start training from previous archive"""
        restored = await self.archiver.restore_from_archive(archive_path)
        
        # Load posteriors into Redis for warm start
        for node_group, data in restored['results']['results'].items():
            if 'posterior' in data:
                group_id = "_".join(map(str, node_group))
                key = f"dist:{group_id}:latest"
                self.trainer.redis.set(
                    key, 
                    data['posterior'].astype(np.float16).tobytes()
                )
    
    async def _load_validated_traces(self) -> pd.DataFrame:
        """Load human-validated traces"""
        # Load from persistent storage
        traces_path = Path("/mnt/validated_traces/latest.parquet")
        if not traces_path.exists():
            raise FileNotFoundError("No validated traces found")
        
        return pd.read_parquet(traces_path)
    
    async def _cleanup_redis_data(self, session_id: str):
        """Clean up Redis data after archival"""
        pattern = f"*{session_id}*"
        cursor = 0
        
        while True:
            cursor, keys = self.trainer.redis.scan(
                cursor, match=pattern, count=1000
            )
            
            if keys:
                self.trainer.redis.delete(*keys)
            
            if cursor == 0:
                break

# Entry point
async def main():
    orchestrator = ProductionTrainingOrchestrator()
    
    # Check for warm start
    latest_archive = Path("/mnt/archives").glob("*.tar.gz")
    latest_archive = max(latest_archive, key=lambda p: p.stat().st_mtime, default=None)
    
    if latest_archive:
        print(f"Warm starting from {latest_archive}")
        await orchestrator.warm_start_from_archive(latest_archive)
    
    # Run training
    result = await orchestrator.run_training_session(mode="mode-training")
    print(f"Training completed: {result}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Docker deployment configuration

```yaml
# docker-compose.yml
version: '3.8'

services:
  redis-primary:
    image: redis:7.2-alpine
    command: >
      redis-server
      --maxmemory 8gb
      --maxmemory-policy allkeys-lru
      --save 60 1000
      --appendonly yes
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    deploy:
      resources:
        limits:
          memory: 10G
        reservations:
          memory: 8G
  
  training-orchestrator:
    build: .
    environment:
      - MODE=mode-training
      - REDIS_URL=redis://redis-primary:6379
      - OLLAMA_HOST=http://ollama:11434
      - MEMORY_THRESHOLD=80
      - MAX_TRAINING_HOURS=24
      - CONVERGENCE_THRESHOLD=0.001
    volumes:
      - ./validated_traces:/mnt/validated_traces:ro
      - ./archives:/mnt/archives
      - training-data:/mnt/training
    depends_on:
      - redis-primary
      - ollama
    deploy:
      resources:
        limits:
          memory: 16G
        reservations:
          memory: 12G
  
  ollama:
    image: ollama/ollama:latest
    volumes:
      - ollama-models:/root/.ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 6G
    command: serve

  # Automated archival service
  archival-cron:
    build: .
    command: python -m archival_service
    environment:
      - ARCHIVE_SCHEDULE="0 */6 * * *"  # Every 6 hours
      - REDIS_URL=redis://redis-primary:6379
    volumes:
      - ./archives:/mnt/archives
      - training-data:/mnt/training
    depends_on:
      - redis-primary

volumes:
  redis-data:
  training-data:
  ollama-models:
```

## Performance optimizations summary

1. **Fastest execution**: Parallel training of all 15 node groups using asyncio, Redis pipelining for atomic operations, float16 embeddings
2. **Stopping criteria**: Convergence detection when improvement < 0.001 for 10 iterations, automatic stopping at 24 hours
3. **Fastest read-write**: Binary Redis storage, Parquet format with zstd compression, memory-mapped numpy arrays
4. **Smallest dataset**: Float16 embeddings (50% reduction), zstd compression level 22, deduplication of common patterns
5. **Automated archival**: Progressive archival every 6 hours, hierarchical tar.gz with ISO 8601 namespaces
6. **Container memory**: Direct cgroup v2/v1 reading, 80% threshold enforcement before agent creation
7. **Architecture flexibility**: Automatic imputation using nearest-neighbor for mismatched configs
8. **Unified optimization**: DSPy MIPROv2 for all tiers with tier-specific hyperparameters

This implementation achieves sub-second latency for trace storage, parallel training across all node combinations, and automatic warm-start capability from previous sessions.
